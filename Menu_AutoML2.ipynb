{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] = 'Gulim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workalendar.asia import SouthKorea\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리\n",
    "- 일자에서 월과 일을 분리\n",
    "- 요일을 레이블 인코딩화(EDA로 요일의 중요도 순 파악)\n",
    "- 월 별, 일 별 중식 석식 수요 차이 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-048b95631755>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  train['주'] = str(pd.DatetimeIndex(train['일자']).week)\n",
      "<ipython-input-4-048b95631755>:4: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  test['주'] = str(pd.DatetimeIndex(test['일자']).week)\n"
     ]
    }
   ],
   "source": [
    "train['월'] = str(pd.DatetimeIndex(train['일자']).month)\n",
    "test['월'] = str(pd.DatetimeIndex(test['일자']).month)\n",
    "train['주'] = str(pd.DatetimeIndex(train['일자']).week)\n",
    "test['주'] = str(pd.DatetimeIndex(test['일자']).week)\n",
    "train['일'] = pd.DatetimeIndex(train['일자']).day\n",
    "test['일'] = pd.DatetimeIndex(test['일자']).day\n",
    "\n",
    "train['출근'] = train['본사정원수']-(train['본사휴가자수']+train['본사출장자수']+train['현본사소속재택근무자수'])\n",
    "train['휴가비율'] = train['본사휴가자수']/train['본사정원수']\n",
    "train['출장비율'] = train['본사출장자수']/train['본사정원수']\n",
    "train['야근비율'] = train['본사시간외근무명령서승인건수']/train['출근']\n",
    "train['재택비율'] = train['현본사소속재택근무자수']/train['본사정원수']\n",
    "\n",
    "test['출근'] = test['본사정원수']-(test['본사휴가자수']+test['본사출장자수']+test['현본사소속재택근무자수'])\n",
    "test['휴가비율'] = test['본사휴가자수']/test['본사정원수']\n",
    "test['출장비율'] = test['본사출장자수']/test['본사정원수']\n",
    "test['야근비율'] = test['본사시간외근무명령서승인건수']/test['출근']\n",
    "test['재택비율'] = test['현본사소속재택근무자수']/test['본사정원수']\n",
    "\n",
    "train['식사가능자수'] = train['본사정원수'] - train['본사휴가자수'] - train['현본사소속재택근무자수']\n",
    "test['식사가능자수'] = test['본사정원수'] - test['본사휴가자수'] - test['현본사소속재택근무자수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_holiday(date):\n",
    "    holidays = list(map(str, pd.Series(np.array(SouthKorea().holidays(int(date[:4])))[:, 0])))\n",
    "    \n",
    "    yesterday = str(np.datetime64(date) - 1)\n",
    "    tomorrow = str(np.datetime64(date) + 1)\n",
    "\n",
    "    if tomorrow in holidays and yesterday in holidays:\n",
    "        return '3'\n",
    "    if tomorrow in holidays:\n",
    "        return '2'\n",
    "    elif yesterday in holidays:\n",
    "        return '1'\n",
    "    else : \n",
    "        return '0'\n",
    "\n",
    "def week_of_month(x):\n",
    "    dt = pendulum.parse(x)\n",
    "    \n",
    "    wom = dt.week_of_month\n",
    "    if wom < 0:\n",
    "        wom += 52\n",
    "    return str(wom)\n",
    "    \n",
    "\n",
    "df = pd.concat([train[['본사정원수', '일자']], test[['본사정원수', '일자']]])\n",
    "df['년월'] = df['일자'].apply(lambda x : x[:7])\n",
    "df = df[['년월', '본사정원수']].groupby(by=['년월'], as_index=False).mean()\n",
    "\n",
    "def member_change(date):\n",
    "    this_month = date[:7]\n",
    "    last_month = str(np.datetime64(this_month) - 1)\n",
    "    \n",
    "    this_month_member = int(df[df['년월'] == this_month]['본사정원수'])\n",
    "    last_month_member = int(df[df['년월'] == last_month]['본사정원수'])\n",
    "    \n",
    "    \n",
    "    return  this_month_member - last_month_member\n",
    "\n",
    "train['공휴일전후'] = train['일자'].apply(is_holiday)\n",
    "test['공휴일전후'] = test['일자'].apply(is_holiday)\n",
    "\n",
    "train['몇주차'] = train['일자'].apply(week_of_month)\n",
    "test['몇주차'] = test['일자'].apply(week_of_month)\n",
    "\n",
    "train = train[train['일자'] > '2016-03']\n",
    "train['인원변화'] = train['일자'].apply(member_change)\n",
    "test['인원변화'] = test['일자'].apply(member_change)\n",
    "\n",
    "train['day2']=0\n",
    "train.loc[train['일']>9, 'day2'] = 1\n",
    "train.loc[train['일']>19, 'day2'] = 2\n",
    "\n",
    "test['day2']=0\n",
    "test.loc[test['일']>9, 'day2'] = 1\n",
    "test.loc[test['일']>19, 'day2'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공휴일 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수',\n",
       "       '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴', '중식계', '석식계', '월', '주', '일',\n",
       "       '출근', '휴가비율', '출장비율', '야근비율', '재택비율', '식사가능자수', '공휴일전후', '몇주차', '인원변화',\n",
       "       'day2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메뉴 변수 없이 사용할떄 해당 코드 사용['공휴일전후', '몇주차', '인원변화']\n",
    "\n",
    "lunch_train = train[['공휴일전후', '몇주차', '인원변화', '요일','월','day2','주','출근', '휴가비율', '출장비율', '야근비율', '재택비율','본사출장자수','본사휴가자수','식사가능자수','본사시간외근무명령서승인건수']]\n",
    "lunch_test = test[['공휴일전후', '몇주차', '인원변화', '요일','월','day2','주','출근', '휴가비율', '출장비율', '야근비율', '재택비율','본사출장자수','본사휴가자수','식사가능자수','본사시간외근무명령서승인건수']]\n",
    "\n",
    "dinner_train= train[['공휴일전후', '몇주차', '인원변화', '요일','월','day2','주','출근', '휴가비율', '출장비율', '야근비율', '재택비율','본사출장자수','본사휴가자수','식사가능자수','본사시간외근무명령서승인건수']]\n",
    "dinner_test = test[['공휴일전후', '몇주차', '인원변화', '요일','월','day2','주','출근', '휴가비율', '출장비율', '야근비율', '재택비율','본사출장자수','본사휴가자수','식사가능자수','본사시간외근무명령서승인건수']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['공휴일전후', '몇주차', '인원변화', '요일', '월', 'day2', '주', '출근', '휴가비율', '출장비율',\n",
       "       '야근비율', '재택비율', '본사출장자수', '본사휴가자수', '식사가능자수', '본사시간외근무명령서승인건수'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunch_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1187, 16)\n",
      "(50, 16)\n"
     ]
    }
   ],
   "source": [
    "print(lunch_train.shape)\n",
    "print(lunch_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1187, 16)\n",
      "(50, 16)\n"
     ]
    }
   ],
   "source": [
    "print(dinner_train.shape)\n",
    "print(dinner_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat features are: ['공휴일전후', '몇주차', '요일', '월', '주']\n",
      "[0 1 3 4 6]\n"
     ]
    }
   ],
   "source": [
    "cat_features = [f for f in lunch_train.columns if lunch_train[f].dtype == 'object']\n",
    "\n",
    "def column_index(df, cat_features):\n",
    "    cols = df.columns.values\n",
    "    sidx = np.argsort(cols)\n",
    "    return sidx[np.searchsorted(cols, cat_features, sorter=sidx)]\n",
    "\n",
    "cat_features_idx = column_index(lunch_train, cat_features)    \n",
    "print(\"Cat features are: %s\" % [f for f in cat_features])\n",
    "print(cat_features_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lunch = train[['중식계']]\n",
    "y_dinner = train[['석식계']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분포 확인 및 분포 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중식 예측모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl_alex import LightGBMRegressor, CatBoostRegressor, AutoMLRegressor\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m14:32:14\u001b[0m | \u001b[1m> Start Fit Base Model\u001b[0m\n",
      "\u001b[32m14:32:33\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:32:33\u001b[0m | \u001b[1m> Start Fit Models 2\u001b[0m\n",
      "\u001b[32m14:32:33\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:32:33\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:32:34\u001b[0m | \u001b[1m> Step 1: calc parameters and pruned score: get test 10 trials\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m One iteration ~ 3.2 sec\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m Possible iters ~ 304.0\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m--------------------------------------------------\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m  Pruned Threshold Score: 76.079\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m> Step 2: Full opt with Threshold Score Pruner\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m> Start optimization with the parameters:\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1mCV_Folds = 12\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1mScore_CV_Folds = 3\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1mFeature_Selection = False\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1mOpt_lvl = 3\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1mCold_start = 15\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1mEarly_stoping = 120\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1mMetric = mean_absolute_error\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1mDirection = minimize\u001b[0m\n",
      "\u001b[32m14:33:06\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "Optimize: : 144it [14:53,  6.21s/it, | Model: LightGBM | OptScore: 68.0271 | Best mean_absolute_error: 66.2408 ]\n",
      "\u001b[32m14:47:59\u001b[0m | \u001b[1m> Finish Opt!\u001b[0m\n",
      "\u001b[32m14:47:59\u001b[0m | \u001b[1mBest Score: 66.2408 mean_absolute_error\u001b[0m\n",
      "\u001b[32m14:47:59\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:47:59\u001b[0m | \u001b[1m> Fit Best Models\u001b[0m\n",
      "\u001b[32m14:47:59\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:48:25\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m14:48:51\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m14:49:14\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m14:49:36\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m14:49:58\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m14:49:59\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:49:59\u001b[0m | \u001b[1m> Finish!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<automl_alex.automl_alex.AutoMLRegressor at 0x24a15aa6c10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunch_model = AutoMLRegressor(random_state=42, metric=MAE)\n",
    "\n",
    "lunch_model.fit(lunch_train, y_lunch,\n",
    "                verbose=3,\n",
    "                folds=12,\n",
    "                opt_lvl=3,\n",
    "                early_stoping=120,\n",
    "                auto_parameters=False,\n",
    "                timeout=1100\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_Auto_lunch = lunch_model.predict(lunch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m14:50:07\u001b[0m | \u001b[1m> Start Fit Base Model\u001b[0m\n",
      "\u001b[32m14:50:26\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:50:26\u001b[0m | \u001b[1m> Start Fit Models 2\u001b[0m\n",
      "\u001b[32m14:50:26\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:50:26\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:50:26\u001b[0m | \u001b[1m> Step 1: calc parameters and pruned score: get test 10 trials\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m One iteration ~ 3.2 sec\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m Possible iters ~ 302.0\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m--------------------------------------------------\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m  Pruned Threshold Score: 72.1806\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m> Step 2: Full opt with Threshold Score Pruner\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m> Start optimization with the parameters:\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1mCV_Folds = 12\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1mScore_CV_Folds = 3\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1mFeature_Selection = False\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1mOpt_lvl = 3\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1mCold_start = 15\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1mEarly_stoping = 120\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1mMetric = mean_absolute_error\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1mDirection = minimize\u001b[0m\n",
      "\u001b[32m14:50:58\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "Optimize: : 214it [14:53,  4.17s/it, | Model: LightGBM | OptScore: 57.1877 | Best mean_absolute_error: 56.8684 ]\n",
      "\u001b[32m15:05:51\u001b[0m | \u001b[1m> Finish Opt!\u001b[0m\n",
      "\u001b[32m15:05:51\u001b[0m | \u001b[1mBest Score: 56.8684 mean_absolute_error\u001b[0m\n",
      "\u001b[32m15:05:51\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m15:05:51\u001b[0m | \u001b[1m> Fit Best Models\u001b[0m\n",
      "\u001b[32m15:05:51\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m15:06:00\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m15:06:15\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m15:06:29\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m15:06:42\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m15:06:56\u001b[0m | \u001b[1mSave DataPrepare\u001b[0m\n",
      "\u001b[32m15:06:58\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m15:06:58\u001b[0m | \u001b[1m> Finish!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<automl_alex.automl_alex.AutoMLRegressor at 0x24a170d3460>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinner_model = AutoMLRegressor(random_state=42, metric=MAE)\n",
    "\n",
    "dinner_model.fit(dinner_train, y_dinner,         \n",
    "                 verbose=3,\n",
    "                 folds=12,\n",
    "                 opt_lvl=3,\n",
    "                 early_stoping=120,\n",
    "                 auto_parameters=False,\n",
    "                 timeout=1100\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['공휴일전후', '몇주차', '인원변화', '요일', '월', 'day2', '주', '출근', '휴가비율', '출장비율',\n",
       "        '야근비율', '재택비율', '본사출장자수', '본사휴가자수', '식사가능자수', '본사시간외근무명령서승인건수'],\n",
       "       dtype='object'),\n",
       " Index(['공휴일전후', '몇주차', '인원변화', '요일', '월', 'day2', '주', '출근', '휴가비율', '출장비율',\n",
       "        '야근비율', '재택비율', '본사출장자수', '본사휴가자수', '식사가능자수', '본사시간외근무명령서승인건수'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunch_train.columns, lunch_test.columns, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_Auto_dinner = dinner_model.predict(dinner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>중식계</th>\n",
       "      <th>석식계</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>1017.253521</td>\n",
       "      <td>368.439814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>919.184360</td>\n",
       "      <td>411.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>617.991654</td>\n",
       "      <td>244.698275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>1267.863473</td>\n",
       "      <td>503.582460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>1064.466851</td>\n",
       "      <td>430.450536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일자          중식계         석식계\n",
       "0  2021-01-27  1017.253521  368.439814\n",
       "1  2021-01-28   919.184360  411.866025\n",
       "2  2021-01-29   617.991654  244.698275\n",
       "3  2021-02-01  1267.863473  503.582460\n",
       "4  2021-02-02  1064.466851  430.450536"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.iloc[:,1] = predicts_Auto_lunch\n",
    "submission.iloc[:,2] = predicts_Auto_dinner\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 날짜 : 20210716\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "today = str(datetime.datetime.now().date()).replace(\"-\",\"\")\n",
    "print(\"오늘 날짜 : \" + today)\n",
    "\n",
    "submission.to_csv(f'../submission/{today}_autoML.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m15:07:04\u001b[0m | \u001b[1m> Start Fit Base Model\u001b[0m\n",
      "\u001b[32m15:07:22\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m15:07:22\u001b[0m | \u001b[1m> Start Fit Models 2\u001b[0m\n",
      "\u001b[32m15:07:22\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m15:07:22\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m15:07:23\u001b[0m | \u001b[1m> Step 1: calc parameters and pruned score: get test 10 trials\u001b[0m\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m One iteration ~ 4.9 sec\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m Possible iters ~ 195.0\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m--------------------------------------------------\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m  Pruned Threshold Score: 136.8445\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m> Step 2: Full opt with Threshold Score Pruner\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m> Start optimization with the parameters:\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1mCV_Folds = 12\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1mScore_CV_Folds = 3\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1mFeature_Selection = False\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1mOpt_lvl = 5\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1mCold_start = 15\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1mEarly_stoping = 120\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1mMetric = mean_absolute_error\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1mDirection = minimize\u001b[0m\n",
      "\u001b[32m15:08:12\u001b[0m | \u001b[1m##################################################\u001b[0m\n",
      "Optimize: : 0it [00:00, ?it/s]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "Optimize: : 1it [00:05,  5.09s/it, | Model: LightGBM | OptScore: 113.0391 | Best mean_absolute_error: 72.7605 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 2it [00:10,  5.33s/it, | Model: LightGBM | OptScore: 71.8612 | Best mean_absolute_error: 72.7605 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "Optimize: : 3it [00:17,  5.93s/it, | Model: LightGBM | OptScore: 78.8537 | Best mean_absolute_error: 71.8612 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "Optimize: : 4it [00:24,  6.39s/it, | Model: LightGBM | OptScore: 77.341 | Best mean_absolute_error: 71.8612 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 5it [00:29,  6.03s/it, | Model: LightGBM | OptScore: 77.8151 | Best mean_absolute_error: 71.8612 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 6it [00:44,  9.07s/it, | Model: LightGBM | OptScore: 77.0506 | Best mean_absolute_error: 71.8612 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 7it [00:48,  7.33s/it, | Model: LightGBM | OptScore: 74.0949 | Best mean_absolute_error: 71.8612 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 8it [00:54,  6.97s/it, | Model: LightGBM | OptScore: 80.142 | Best mean_absolute_error: 71.8612 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 9it [01:04,  8.03s/it, | Model: LightGBM | OptScore: 71.9867 | Best mean_absolute_error: 71.8612 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimize: : 10it [01:19, 10.15s/it, | Model: LightGBM | OptScore: 72.569 | Best mean_absolute_error: 71.8612 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 11it [01:33, 11.20s/it, | Model: LightGBM | OptScore: 71.1919 | Best mean_absolute_error: 71.8612 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 12it [01:42, 10.58s/it, | Model: LightGBM | OptScore: 71.191 | Best mean_absolute_error: 71.1919 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 13it [01:46,  8.68s/it, | Model: LightGBM | OptScore: 73.3977 | Best mean_absolute_error: 71.191 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 14it [01:55,  8.56s/it, | Model: LightGBM | OptScore: 70.9507 | Best mean_absolute_error: 71.191 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 15it [02:05,  9.11s/it, | Model: LightGBM | OptScore: 71.8509 | Best mean_absolute_error: 70.9507 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 16it [02:10,  7.82s/it, | Model: LightGBM | OptScore: 72.7701 | Best mean_absolute_error: 70.9507 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 17it [02:17,  7.64s/it, | Model: LightGBM | OptScore: 73.9433 | Best mean_absolute_error: 70.9507 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 18it [02:27,  8.28s/it, | Model: LightGBM | OptScore: 69.916 | Best mean_absolute_error: 70.9507 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 19it [02:32,  7.32s/it, | Model: LightGBM | OptScore: 72.4737 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 20it [02:38,  6.77s/it, | Model: LightGBM | OptScore: 72.4359 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 21it [02:48,  7.95s/it, | Model: LightGBM | OptScore: 71.6784 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 22it [03:00,  9.20s/it, | Model: LightGBM | OptScore: 72.1976 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimize: : 23it [03:08,  8.85s/it, | Model: LightGBM | OptScore: 69.9251 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 24it [03:12,  7.23s/it, | Model: LightGBM | OptScore: 70.8325 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 25it [03:16,  6.37s/it, | Model: LightGBM | OptScore: 70.644 | Best mean_absolute_error: 69.916 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 26it [03:19,  5.39s/it, | Model: LightGBM | OptScore: 72.9893 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 27it [03:26,  5.67s/it, | Model: LightGBM | OptScore: 71.7028 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 28it [03:30,  5.35s/it, | Model: LightGBM | OptScore: 71.0056 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 29it [03:33,  4.63s/it, | Model: LightGBM | OptScore: 68.8762 | Best mean_absolute_error: 69.916 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 30it [03:36,  4.13s/it, | Model: LightGBM | OptScore: 71.6209 | Best mean_absolute_error: 68.8762 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 31it [03:39,  3.69s/it, | Model: LightGBM | OptScore: 81.9567 | Best mean_absolute_error: 68.8762 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 32it [03:45,  4.58s/it, | Model: LightGBM | OptScore: 71.2181 | Best mean_absolute_error: 68.8762 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 33it [03:47,  3.69s/it, | Model: LightGBM | OptScore: 80.7337 | Best mean_absolute_error: 68.8762 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 34it [03:51,  3.89s/it, | Model: LightGBM | OptScore: 71.2343 | Best mean_absolute_error: 68.8762 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 35it [04:00,  5.41s/it, | Model: LightGBM | OptScore: 72.4452 | Best mean_absolute_error: 68.8762 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 36it [04:03,  4.66s/it, | Model: LightGBM | OptScore: 68.7868 | Best mean_absolute_error: 68.8762 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 37it [04:06,  4.18s/it, | Model: LightGBM | OptScore: 69.9872 | Best mean_absolute_error: 68.7868 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 38it [04:09,  3.59s/it, | Model: LightGBM | OptScore: 68.6635 | Best mean_absolute_error: 68.7868 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 39it [04:11,  3.14s/it, | Model: LightGBM | OptScore: 69.4757 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 40it [04:12,  2.75s/it, | Model: LightGBM | OptScore: 71.1143 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 41it [04:15,  2.60s/it, | Model: LightGBM | OptScore: 69.0957 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 42it [04:17,  2.37s/it, | Model: LightGBM | OptScore: 79.4573 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 43it [04:19,  2.25s/it, | Model: LightGBM | OptScore: 70.2176 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "Optimize: : 44it [04:21,  2.24s/it, | Model: LightGBM | OptScore: 72.4237 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 45it [04:23,  2.24s/it, | Model: LightGBM | OptScore: 73.3193 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimize: : 46it [04:26,  2.43s/it, | Model: LightGBM | OptScore: 69.5472 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 47it [04:29,  2.54s/it, | Model: LightGBM | OptScore: 69.1399 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 48it [04:32,  2.75s/it, | Model: LightGBM | OptScore: 69.8086 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 49it [04:33,  2.37s/it, | Model: LightGBM | OptScore: 71.0379 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 50it [04:36,  2.41s/it, | Model: LightGBM | OptScore: 76.7979 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 51it [04:39,  2.68s/it, | Model: LightGBM | OptScore: 101.4295 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 52it [04:41,  2.51s/it, | Model: LightGBM | OptScore: 69.8614 | Best mean_absolute_error: 68.6635 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 53it [04:43,  2.39s/it, | Model: LightGBM | OptScore: 68.5366 | Best mean_absolute_error: 68.6635 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 54it [04:45,  2.24s/it, | Model: LightGBM | OptScore: 73.7436 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 55it [04:47,  2.06s/it, | Model: LightGBM | OptScore: 71.8315 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 56it [04:49,  1.95s/it, | Model: LightGBM | OptScore: 69.6925 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 57it [04:51,  2.02s/it, | Model: LightGBM | OptScore: 70.891 | Best mean_absolute_error: 68.5366 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 58it [04:54,  2.32s/it, | Model: LightGBM | OptScore: 71.9795 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 59it [04:56,  2.14s/it, | Model: LightGBM | OptScore: 70.9155 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimize: : 60it [04:58,  2.08s/it, | Model: LightGBM | OptScore: 71.7844 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 61it [05:00,  2.18s/it, | Model: LightGBM | OptScore: 72.3143 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 62it [05:02,  2.24s/it, | Model: LightGBM | OptScore: 70.5486 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 63it [05:05,  2.26s/it, | Model: LightGBM | OptScore: 69.7467 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 64it [05:06,  2.15s/it, | Model: LightGBM | OptScore: 72.0456 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 65it [05:10,  2.70s/it, | Model: LightGBM | OptScore: 69.492 | Best mean_absolute_error: 68.5366 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 66it [05:12,  2.44s/it, | Model: LightGBM | OptScore: 70.0797 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 67it [05:14,  2.26s/it, | Model: LightGBM | OptScore: 71.0303 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 68it [05:17,  2.31s/it, | Model: LightGBM | OptScore: 67.7803 | Best mean_absolute_error: 68.5366 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 69it [05:18,  2.17s/it, | Model: LightGBM | OptScore: 73.1626 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 70it [05:20,  2.07s/it, | Model: LightGBM | OptScore: 70.4147 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 71it [05:22,  2.00s/it, | Model: LightGBM | OptScore: 70.8628 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 72it [05:24,  2.05s/it, | Model: LightGBM | OptScore: 126.0273 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimize: : 73it [05:26,  1.87s/it, | Model: LightGBM | OptScore: 73.4019 | Best mean_absolute_error: 67.7803 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 74it [05:27,  1.77s/it, | Model: LightGBM | OptScore: 71.5251 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 75it [05:29,  1.73s/it, | Model: LightGBM | OptScore: 69.405 | Best mean_absolute_error: 67.7803 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 76it [05:31,  1.84s/it, | Model: LightGBM | OptScore: 70.5296 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 77it [05:33,  1.84s/it, | Model: LightGBM | OptScore: 70.5967 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 78it [05:35,  1.85s/it, | Model: LightGBM | OptScore: 70.8637 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 79it [05:37,  1.87s/it, | Model: LightGBM | OptScore: 71.0646 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 80it [05:38,  1.88s/it, | Model: LightGBM | OptScore: 70.958 | Best mean_absolute_error: 67.7803 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 81it [05:41,  2.06s/it, | Model: LightGBM | OptScore: 71.8845 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 82it [05:43,  1.98s/it, | Model: LightGBM | OptScore: 71.081 | Best mean_absolute_error: 67.7803 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 83it [05:46,  2.33s/it, | Model: LightGBM | OptScore: 70.8263 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 84it [05:48,  2.23s/it, | Model: LightGBM | OptScore: 72.0901 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 85it [05:50,  2.12s/it, | Model: LightGBM | OptScore: 70.7002 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimize: : 86it [05:52,  2.10s/it, | Model: LightGBM | OptScore: 71.2792 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 87it [05:53,  1.93s/it, | Model: LightGBM | OptScore: 72.4398 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 88it [05:55,  1.89s/it, | Model: LightGBM | OptScore: 72.3263 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 89it [05:57,  1.85s/it, | Model: LightGBM | OptScore: 70.5275 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 90it [06:00,  2.28s/it, | Model: LightGBM | OptScore: 74.2503 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 91it [06:02,  2.07s/it, | Model: LightGBM | OptScore: 74.4719 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 92it [06:04,  1.99s/it, | Model: LightGBM | OptScore: 71.1758 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 93it [06:05,  1.95s/it, | Model: LightGBM | OptScore: 70.4694 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 94it [06:07,  1.87s/it, | Model: LightGBM | OptScore: 70.7815 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 95it [06:09,  1.89s/it, | Model: LightGBM | OptScore: 71.3485 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 96it [06:11,  1.88s/it, | Model: LightGBM | OptScore: 68.7075 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 97it [06:13,  1.89s/it, | Model: LightGBM | OptScore: 68.7985 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 98it [06:15,  1.93s/it, | Model: LightGBM | OptScore: 69.2831 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimize: : 99it [06:17,  1.90s/it, | Model: LightGBM | OptScore: 121.1278 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 100it [06:19,  1.96s/it, | Model: LightGBM | OptScore: 71.6607 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 101it [06:20,  1.90s/it, | Model: LightGBM | OptScore: 70.9592 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 102it [06:23,  2.07s/it, | Model: LightGBM | OptScore: 74.32 | Best mean_absolute_error: 67.7803 ]  C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 103it [06:25,  2.10s/it, | Model: LightGBM | OptScore: 70.4066 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 104it [06:27,  2.07s/it, | Model: LightGBM | OptScore: 71.507 | Best mean_absolute_error: 67.7803 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 105it [06:29,  1.99s/it, | Model: LightGBM | OptScore: 71.2079 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 106it [06:31,  2.00s/it, | Model: LightGBM | OptScore: 71.1111 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 107it [06:33,  1.93s/it, | Model: LightGBM | OptScore: 69.5797 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 108it [06:35,  1.90s/it, | Model: LightGBM | OptScore: 71.691 | Best mean_absolute_error: 67.7803 ] C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 109it [06:37,  2.02s/it, | Model: LightGBM | OptScore: 70.492 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 110it [06:39,  1.92s/it, | Model: LightGBM | OptScore: 70.768 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 111it [06:42,  2.41s/it, | Model: LightGBM | OptScore: 71.6515 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimize: : 112it [06:44,  2.30s/it, | Model: LightGBM | OptScore: 71.8758 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 113it [06:46,  2.21s/it, | Model: LightGBM | OptScore: 70.9278 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 114it [06:48,  2.16s/it, | Model: LightGBM | OptScore: 72.0139 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 115it [06:50,  2.07s/it, | Model: LightGBM | OptScore: 71.9083 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 116it [06:52,  2.02s/it, | Model: LightGBM | OptScore: 70.4115 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 117it [06:54,  1.99s/it, | Model: LightGBM | OptScore: 69.5276 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 118it [06:56,  2.05s/it, | Model: LightGBM | OptScore: 80.6553 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "Optimize: : 119it [06:58,  1.96s/it, | Model: LightGBM | OptScore: 72.8929 | Best mean_absolute_error: 67.7803 ]C:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\trial\\_trial.py:769: RuntimeWarning: Inconsistent parameter values for distribution with name \"lgbm_learning_rate\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 0.01, 'high': 0.3}\n",
      "  warnings.warn(\n",
      "Optimize: : 119it [06:59,  3.52s/it, | Model: LightGBM | OptScore: 72.8929 | Best mean_absolute_error: 67.7803 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d9c389808c85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlunch_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoMLRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m lunch_model.fit(lunch_train, y_lunch,\n\u001b[0m\u001b[0;32m      4\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\automl_alex\\automl_alex.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, timeout, auto_parameters, folds, score_folds, opt_lvl, early_stoping, feature_selection, verbose)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[0mtimeout_model_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_step_0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m         history = self.model_2.opt(\n\u001b[0m\u001b[0;32m    446\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\automl_alex\\optimizer.py\u001b[0m in \u001b[0;36mopt\u001b[1;34m(self, X, y, timeout, verbose, fit_end)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Optimize: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m                 self.study.optimize(\n\u001b[0m\u001b[0;32m    667\u001b[0m                     \u001b[1;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mobj_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_step2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    399\u001b[0m             )\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    402\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     66\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\automl_alex\\optimizer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                 self.study.optimize(\n\u001b[1;32m--> 667\u001b[1;33m                     \u001b[1;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mobj_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_step2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\automl_alex\\optimizer.py\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial, self, X, y, step, return_model, verbose)\u001b[0m\n\u001b[0;32m    534\u001b[0m         ):\n\u001b[0;32m    535\u001b[0m             \u001b[1;31m# self._set_opt_sys_info()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m             score, score_std = self._opt_objective(\n\u001b[0m\u001b[0;32m    537\u001b[0m                 \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\automl_alex\\optimizer.py\u001b[0m in \u001b[0;36m_opt_objective\u001b[1;34m(self, trial, X, y, return_model, verbose)\u001b[0m\n\u001b[0;32m    456\u001b[0m                 )\n\u001b[0;32m    457\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m                 \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\automl_alex\\cross_validation.py\u001b[0m in \u001b[0;36mfit_score\u001b[1;34m(self, X, y, cat_features, print_metric, trial)\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m             score_model = self.estimator.fit_score(\n\u001b[0m\u001b[0;32m    352\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m                 \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\automl_alex\\_base.py\u001b[0m in \u001b[0;36mfit_score\u001b[1;34m(self, X_train, y_train, X_test, y_test, cat_features, metric, print_metric, metric_round)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         self.fit(\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\automl_alex\\models\\model_lightgbm.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, cat_features)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mcat_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         self.model = lgb.train(\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mmodel_param\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   2641\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2642\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot update due to null objective function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2643\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[0;32m   2644\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2645\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lunch_model = AutoMLRegressor(random_state=42, metric=MAE)\n",
    "\n",
    "lunch_model.fit(lunch_train, y_lunch,\n",
    "                verbose=3,\n",
    "                folds=12,\n",
    "                opt_lvl=5,\n",
    "                early_stoping=120,\n",
    "                auto_parameters=False,\n",
    "                timeout=1100\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_Auto_lunch_5 = lunch_model.predict(lunch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinner_model = AutoMLRegressor(random_state=42, metric=MAE)\n",
    "\n",
    "dinner_model.fit(dinner_train, y_dinner,         \n",
    "                 verbose=3,\n",
    "                 folds=12,\n",
    "                 opt_lvl=5,\n",
    "                 early_stoping=120,\n",
    "                 auto_parameters=False,\n",
    "                 timeout=1100\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_Auto_dinner_5 = dinner_model.predict(dinner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2 = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "submission2.iloc[:,1] = predicts_Auto_lunch_5\n",
    "submission2.iloc[:,2] = predicts_Auto_dinner_5\n",
    "submission2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2.to_csv(f'../submission/{today}_autoML_5.csv', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
